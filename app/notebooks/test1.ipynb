{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb3644a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting nltk\n",
      "  Downloading nltk-3.9.1-py3-none-any.whl.metadata (2.9 kB)\n",
      "Collecting rouge-score\n",
      "  Downloading rouge_score-0.1.2.tar.gz (17 kB)\n",
      "  Installing build dependencies: started\n",
      "  Installing build dependencies: finished with status 'done'\n",
      "  Getting requirements to build wheel: started\n",
      "  Getting requirements to build wheel: finished with status 'done'\n",
      "  Preparing metadata (pyproject.toml): started\n",
      "  Preparing metadata (pyproject.toml): finished with status 'done'\n",
      "Collecting bert_score\n",
      "  Downloading bert_score-0.3.13-py3-none-any.whl.metadata (15 kB)\n",
      "Requirement already satisfied: sentence-transformers in c:\\users\\tahmi\\documents\\work\\semanticllm25\\llm-similarity\\.venv\\lib\\site-packages (4.1.0)\n",
      "Collecting sacrebleu\n",
      "  Downloading sacrebleu-2.5.1-py3-none-any.whl.metadata (51 kB)\n",
      "Requirement already satisfied: transformers in c:\\users\\tahmi\\documents\\work\\semanticllm25\\llm-similarity\\.venv\\lib\\site-packages (4.52.3)\n",
      "Requirement already satisfied: torch in c:\\users\\tahmi\\documents\\work\\semanticllm25\\llm-similarity\\.venv\\lib\\site-packages (2.7.0)\n",
      "Collecting accelerate\n",
      "  Using cached accelerate-1.7.0-py3-none-any.whl.metadata (19 kB)\n",
      "Requirement already satisfied: click in c:\\users\\tahmi\\documents\\work\\semanticllm25\\llm-similarity\\.venv\\lib\\site-packages (from nltk) (8.2.1)\n",
      "Requirement already satisfied: joblib in c:\\users\\tahmi\\documents\\work\\semanticllm25\\llm-similarity\\.venv\\lib\\site-packages (from nltk) (1.5.0)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\tahmi\\documents\\work\\semanticllm25\\llm-similarity\\.venv\\lib\\site-packages (from nltk) (2024.11.6)\n",
      "Requirement already satisfied: tqdm in c:\\users\\tahmi\\documents\\work\\semanticllm25\\llm-similarity\\.venv\\lib\\site-packages (from nltk) (4.67.1)\n",
      "Collecting absl-py (from rouge-score)\n",
      "  Using cached absl_py-2.2.2-py3-none-any.whl.metadata (2.6 kB)\n",
      "Requirement already satisfied: numpy in c:\\users\\tahmi\\documents\\work\\semanticllm25\\llm-similarity\\.venv\\lib\\site-packages (from rouge-score) (2.2.6)\n",
      "Requirement already satisfied: six>=1.14.0 in c:\\users\\tahmi\\documents\\work\\semanticllm25\\llm-similarity\\.venv\\lib\\site-packages (from rouge-score) (1.17.0)\n",
      "Requirement already satisfied: pandas>=1.0.1 in c:\\users\\tahmi\\documents\\work\\semanticllm25\\llm-similarity\\.venv\\lib\\site-packages (from bert_score) (2.2.3)\n",
      "Requirement already satisfied: requests in c:\\users\\tahmi\\documents\\work\\semanticllm25\\llm-similarity\\.venv\\lib\\site-packages (from bert_score) (2.32.3)\n",
      "Collecting matplotlib (from bert_score)\n",
      "  Using cached matplotlib-3.10.3-cp312-cp312-win_amd64.whl.metadata (11 kB)\n",
      "Requirement already satisfied: packaging>=20.9 in c:\\users\\tahmi\\documents\\work\\semanticllm25\\llm-similarity\\.venv\\lib\\site-packages (from bert_score) (24.2)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\tahmi\\documents\\work\\semanticllm25\\llm-similarity\\.venv\\lib\\site-packages (from sentence-transformers) (1.6.1)\n",
      "Requirement already satisfied: scipy in c:\\users\\tahmi\\documents\\work\\semanticllm25\\llm-similarity\\.venv\\lib\\site-packages (from sentence-transformers) (1.15.3)\n",
      "Requirement already satisfied: huggingface-hub>=0.20.0 in c:\\users\\tahmi\\documents\\work\\semanticllm25\\llm-similarity\\.venv\\lib\\site-packages (from sentence-transformers) (0.31.4)\n",
      "Requirement already satisfied: Pillow in c:\\users\\tahmi\\documents\\work\\semanticllm25\\llm-similarity\\.venv\\lib\\site-packages (from sentence-transformers) (11.2.1)\n",
      "Requirement already satisfied: typing_extensions>=4.5.0 in c:\\users\\tahmi\\documents\\work\\semanticllm25\\llm-similarity\\.venv\\lib\\site-packages (from sentence-transformers) (4.13.2)\n",
      "Collecting portalocker (from sacrebleu)\n",
      "  Downloading portalocker-3.1.1-py3-none-any.whl.metadata (8.6 kB)\n",
      "Collecting tabulate>=0.8.9 (from sacrebleu)\n",
      "  Downloading tabulate-0.9.0-py3-none-any.whl.metadata (34 kB)\n",
      "Requirement already satisfied: colorama in c:\\users\\tahmi\\documents\\work\\semanticllm25\\llm-similarity\\.venv\\lib\\site-packages (from sacrebleu) (0.4.6)\n",
      "Collecting lxml (from sacrebleu)\n",
      "  Downloading lxml-5.4.0-cp312-cp312-win_amd64.whl.metadata (3.6 kB)\n",
      "Requirement already satisfied: filelock in c:\\users\\tahmi\\documents\\work\\semanticllm25\\llm-similarity\\.venv\\lib\\site-packages (from transformers) (3.18.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\tahmi\\documents\\work\\semanticllm25\\llm-similarity\\.venv\\lib\\site-packages (from transformers) (6.0.2)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in c:\\users\\tahmi\\documents\\work\\semanticllm25\\llm-similarity\\.venv\\lib\\site-packages (from transformers) (0.21.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in c:\\users\\tahmi\\documents\\work\\semanticllm25\\llm-similarity\\.venv\\lib\\site-packages (from transformers) (0.5.3)\n",
      "Requirement already satisfied: sympy>=1.13.3 in c:\\users\\tahmi\\documents\\work\\semanticllm25\\llm-similarity\\.venv\\lib\\site-packages (from torch) (1.14.0)\n",
      "Requirement already satisfied: networkx in c:\\users\\tahmi\\documents\\work\\semanticllm25\\llm-similarity\\.venv\\lib\\site-packages (from torch) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\tahmi\\documents\\work\\semanticllm25\\llm-similarity\\.venv\\lib\\site-packages (from torch) (3.1.6)\n",
      "Requirement already satisfied: fsspec in c:\\users\\tahmi\\documents\\work\\semanticllm25\\llm-similarity\\.venv\\lib\\site-packages (from torch) (2025.5.0)\n",
      "Requirement already satisfied: setuptools in c:\\users\\tahmi\\documents\\work\\semanticllm25\\llm-similarity\\.venv\\lib\\site-packages (from torch) (80.8.0)\n",
      "Requirement already satisfied: psutil in c:\\users\\tahmi\\documents\\work\\semanticllm25\\llm-similarity\\.venv\\lib\\site-packages (from accelerate) (7.0.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\tahmi\\documents\\work\\semanticllm25\\llm-similarity\\.venv\\lib\\site-packages (from pandas>=1.0.1->bert_score) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\tahmi\\documents\\work\\semanticllm25\\llm-similarity\\.venv\\lib\\site-packages (from pandas>=1.0.1->bert_score) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\tahmi\\documents\\work\\semanticllm25\\llm-similarity\\.venv\\lib\\site-packages (from pandas>=1.0.1->bert_score) (2025.2)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\tahmi\\documents\\work\\semanticllm25\\llm-similarity\\.venv\\lib\\site-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\tahmi\\documents\\work\\semanticllm25\\llm-similarity\\.venv\\lib\\site-packages (from jinja2->torch) (3.0.2)\n",
      "Collecting contourpy>=1.0.1 (from matplotlib->bert_score)\n",
      "  Using cached contourpy-1.3.2-cp312-cp312-win_amd64.whl.metadata (5.5 kB)\n",
      "Collecting cycler>=0.10 (from matplotlib->bert_score)\n",
      "  Using cached cycler-0.12.1-py3-none-any.whl.metadata (3.8 kB)\n",
      "Collecting fonttools>=4.22.0 (from matplotlib->bert_score)\n",
      "  Using cached fonttools-4.58.0-cp312-cp312-win_amd64.whl.metadata (106 kB)\n",
      "Collecting kiwisolver>=1.3.1 (from matplotlib->bert_score)\n",
      "  Using cached kiwisolver-1.4.8-cp312-cp312-win_amd64.whl.metadata (6.3 kB)\n",
      "Collecting pyparsing>=2.3.1 (from matplotlib->bert_score)\n",
      "  Using cached pyparsing-3.2.3-py3-none-any.whl.metadata (5.0 kB)\n",
      "Requirement already satisfied: pywin32>=226 in c:\\users\\tahmi\\documents\\work\\semanticllm25\\llm-similarity\\.venv\\lib\\site-packages (from portalocker->sacrebleu) (310)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\tahmi\\documents\\work\\semanticllm25\\llm-similarity\\.venv\\lib\\site-packages (from requests->bert_score) (3.4.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\tahmi\\documents\\work\\semanticllm25\\llm-similarity\\.venv\\lib\\site-packages (from requests->bert_score) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\tahmi\\documents\\work\\semanticllm25\\llm-similarity\\.venv\\lib\\site-packages (from requests->bert_score) (2.4.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\tahmi\\documents\\work\\semanticllm25\\llm-similarity\\.venv\\lib\\site-packages (from requests->bert_score) (2025.4.26)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\users\\tahmi\\documents\\work\\semanticllm25\\llm-similarity\\.venv\\lib\\site-packages (from scikit-learn->sentence-transformers) (3.6.0)\n",
      "Downloading nltk-3.9.1-py3-none-any.whl (1.5 MB)\n",
      "   ---------------------------------------- 0.0/1.5 MB ? eta -:--:--\n",
      "   ------ --------------------------------- 0.3/1.5 MB ? eta -:--:--\n",
      "   --------------------------- ------------ 1.0/1.5 MB 3.6 MB/s eta 0:00:01\n",
      "   ---------------------------------- ----- 1.3/1.5 MB 3.7 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 1.5/1.5 MB 2.7 MB/s eta 0:00:00\n",
      "Downloading bert_score-0.3.13-py3-none-any.whl (61 kB)\n",
      "Downloading sacrebleu-2.5.1-py3-none-any.whl (104 kB)\n",
      "Using cached accelerate-1.7.0-py3-none-any.whl (362 kB)\n",
      "Downloading tabulate-0.9.0-py3-none-any.whl (35 kB)\n",
      "Using cached absl_py-2.2.2-py3-none-any.whl (135 kB)\n",
      "Downloading lxml-5.4.0-cp312-cp312-win_amd64.whl (3.8 MB)\n",
      "   ---------------------------------------- 0.0/3.8 MB ? eta -:--:--\n",
      "   ---------- ----------------------------- 1.0/3.8 MB 5.6 MB/s eta 0:00:01\n",
      "   --------------------- ------------------ 2.1/3.8 MB 4.9 MB/s eta 0:00:01\n",
      "   --------------------------- ------------ 2.6/3.8 MB 5.2 MB/s eta 0:00:01\n",
      "   -------------------------------- ------- 3.1/3.8 MB 3.8 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 3.8/3.8 MB 3.8 MB/s eta 0:00:00\n",
      "Using cached matplotlib-3.10.3-cp312-cp312-win_amd64.whl (8.1 MB)\n",
      "Downloading portalocker-3.1.1-py3-none-any.whl (19 kB)\n",
      "Using cached contourpy-1.3.2-cp312-cp312-win_amd64.whl (223 kB)\n",
      "Using cached cycler-0.12.1-py3-none-any.whl (8.3 kB)\n",
      "Using cached fonttools-4.58.0-cp312-cp312-win_amd64.whl (2.2 MB)\n",
      "Using cached kiwisolver-1.4.8-cp312-cp312-win_amd64.whl (71 kB)\n",
      "Using cached pyparsing-3.2.3-py3-none-any.whl (111 kB)\n",
      "Building wheels for collected packages: rouge-score\n",
      "  Building wheel for rouge-score (pyproject.toml): started\n",
      "  Building wheel for rouge-score (pyproject.toml): finished with status 'done'\n",
      "  Created wheel for rouge-score: filename=rouge_score-0.1.2-py3-none-any.whl size=25024 sha256=176ff9c507ce969ab203dac6468e25b4365fbc63734fe761f39996a497dbf2f9\n",
      "  Stored in directory: c:\\users\\tahmi\\appdata\\local\\pip\\cache\\wheels\\85\\9d\\af\\01feefbe7d55ef5468796f0c68225b6788e85d9d0a281e7a70\n",
      "Successfully built rouge-score\n",
      "Installing collected packages: tabulate, pyparsing, portalocker, lxml, kiwisolver, fonttools, cycler, contourpy, absl-py, sacrebleu, nltk, matplotlib, rouge-score, accelerate, bert_score\n",
      "Successfully installed absl-py-2.2.2 accelerate-1.7.0 bert_score-0.3.13 contourpy-1.3.2 cycler-0.12.1 fonttools-4.58.0 kiwisolver-1.4.8 lxml-5.4.0 matplotlib-3.10.3 nltk-3.9.1 portalocker-3.1.1 pyparsing-3.2.3 rouge-score-0.1.2 sacrebleu-2.5.1 tabulate-0.9.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 25.0.1 -> 25.1.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "# 0. Setup - Run this cell first if you don't have the libraries installed\n",
    "%pip install nltk rouge-score bert_score sentence-transformers sacrebleu transformers torch accelerate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b9b83ad0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# # NLTK downloads (run once in Python console or a separate cell)\n",
    "import nltk\n",
    "nltk.download('punkt', quiet=True)\n",
    "nltk.download('wordnet', quiet=True)\n",
    "nltk.download('omw-1.4', quiet=True) # Needed for WordNet in some NLTK versions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eab7e5dd",
   "metadata": {},
   "source": [
    "# Import Libs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f51a7919",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\tahmi\\Documents\\Work\\semanticLLM25\\llm-similarity\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Libraries imported successfully!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
    "from nltk.translate.meteor_score import single_meteor_score # Note: NLTK's meteor is single_meteor_score\n",
    "from rouge_score import rouge_scorer\n",
    "import sacrebleu # For chrF\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "from bert_score import score as bert_score_calc\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, pipeline\n",
    "import torch\n",
    "import re\n",
    "import pandas as pd\n",
    "\n",
    "print(\"Libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53a26b3d",
   "metadata": {},
   "source": [
    "# Define sample sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d8f9a7f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source Sentence: The cat sat on the mat.\n",
      "Target Sentences: ['A feline was resting on the rug.', 'The cat was on the mat.', 'There is a cat on the mat.', 'The dog chased the ball.', 'Weather is pleasant today.', 'Le chat est assis sur le tapis.']\n"
     ]
    }
   ],
   "source": [
    "source_sentence = \"The cat sat on the mat.\"\n",
    "\n",
    "target_sentences = [\n",
    "    \"A feline was resting on the rug.\",                            # High similarity\n",
    "    \"The cat was on the mat.\",                                   # Very high similarity, slight variation\n",
    "    \"There is a cat on the mat.\",                                # High similarity\n",
    "    \"The dog chased the ball.\",                                  # Low similarity\n",
    "    \"Weather is pleasant today.\",                                # No similarity\n",
    "    \"Le chat est assis sur le tapis.\"                            # French translation - for some metrics to show 0\n",
    "]\n",
    "\n",
    "print(\"Source Sentence:\", source_sentence)\n",
    "print(\"Target Sentences:\", target_sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bcdd6a7",
   "metadata": {},
   "source": [
    "# Preprocessing Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0c6b78da",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_text(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'\\s+', ' ', text).strip() # Normalize whitespace\n",
    "    return text\n",
    "\n",
    "normalized_source = normalize_text(source_sentence)\n",
    "normalized_targets = [normalize_text(t) for t in target_sentences]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58126102",
   "metadata": {},
   "source": [
    "# Lexical Sim. Metrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "472ce73b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- BLEU Scores ---\n",
      "Target: \"A feline was resting on the rug.\" -> BLEU: 0.0699\n",
      "Target: \"The cat was on the mat.\" -> BLEU: 0.2541\n",
      "Target: \"There is a cat on the mat.\" -> BLEU: 0.1757\n",
      "Target: \"The dog chased the ball.\" -> BLEU: 0.0523\n",
      "Target: \"Weather is pleasant today.\" -> BLEU: 0.0000\n",
      "Target: \"Le chat est assis sur le tapis.\" -> BLEU: 0.0000\n"
     ]
    }
   ],
   "source": [
    "# 4.1 BLEU (Bilingual Evaluation Understudy)\n",
    "# Measures precision of n-grams. Output is 0-1 (higher is better).\n",
    "# NLTK's sentence_bleu expects tokenized input.\n",
    "print(\"\\n--- BLEU Scores ---\")\n",
    "smoothie = SmoothingFunction().method1 # Smoothing for short sentences\n",
    "\n",
    "for target in target_sentences:\n",
    "    tokenized_source = normalized_source.split() # Using normalized for consistency here\n",
    "    tokenized_target = normalize_text(target).split()\n",
    "    bleu_score = sentence_bleu([tokenized_source], tokenized_target, smoothing_function=smoothie)\n",
    "    print(f\"Target: \\\"{target}\\\" -> BLEU: {bleu_score:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65ea3204",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- ROUGE-L F-scores ---\n",
      "Target: \"A feline was resting on the rug.\" -> ROUGE-L F1: 0.3077\n",
      "Target: \"The cat was on the mat.\" -> ROUGE-L F1: 0.8333\n",
      "Target: \"There is a cat on the mat.\" -> ROUGE-L F1: 0.6154\n",
      "Target: \"The dog chased the ball.\" -> ROUGE-L F1: 0.3636\n",
      "Target: \"Weather is pleasant today.\" -> ROUGE-L F1: 0.0000\n",
      "Target: \"Le chat est assis sur le tapis.\" -> ROUGE-L F1: 0.0000\n"
     ]
    }
   ],
   "source": [
    "# 4.2 ROUGE (Recall-Oriented Understudy for Gisting Evaluation)\n",
    "# Measures recall of n-grams. We'll look at ROUGE-L F-score. Output is 0-1.\n",
    "print(\"\\n--- ROUGE-L F-scores ---\")\n",
    "scorer = rouge_scorer.RougeScorer(['rougeL'], use_stemmer=True)\n",
    "\n",
    "for target in target_sentences:\n",
    "    # ROUGE scorer expects untokenized strings\n",
    "    scores = scorer.score(source_sentence, target)\n",
    "    print(f\"Target: \\\"{target}\\\" -> ROUGE-L F1: {scores['rougeL'].fmeasure:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d42ae26f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- METEOR Scores ---\n",
      "Note: METEOR encountered an issue with 'A feline was resting on the rug.', score set to 0. Error: \"hypothesis\" expects pre-tokenized hypothesis (Iterable[str]): A feline was resting on the rug.\n",
      "Target: \"A feline was resting on the rug.\" -> METEOR: 0.0000\n",
      "Note: METEOR encountered an issue with 'The cat was on the mat.', score set to 0. Error: \"hypothesis\" expects pre-tokenized hypothesis (Iterable[str]): The cat was on the mat.\n",
      "Target: \"The cat was on the mat.\" -> METEOR: 0.0000\n",
      "Note: METEOR encountered an issue with 'There is a cat on the mat.', score set to 0. Error: \"hypothesis\" expects pre-tokenized hypothesis (Iterable[str]): There is a cat on the mat.\n",
      "Target: \"There is a cat on the mat.\" -> METEOR: 0.0000\n",
      "Note: METEOR encountered an issue with 'The dog chased the ball.', score set to 0. Error: \"hypothesis\" expects pre-tokenized hypothesis (Iterable[str]): The dog chased the ball.\n",
      "Target: \"The dog chased the ball.\" -> METEOR: 0.0000\n",
      "Note: METEOR encountered an issue with 'Weather is pleasant today.', score set to 0. Error: \"hypothesis\" expects pre-tokenized hypothesis (Iterable[str]): Weather is pleasant today.\n",
      "Target: \"Weather is pleasant today.\" -> METEOR: 0.0000\n",
      "Note: METEOR encountered an issue with 'Le chat est assis sur le tapis.', score set to 0. Error: \"hypothesis\" expects pre-tokenized hypothesis (Iterable[str]): Le chat est assis sur le tapis.\n",
      "Target: \"Le chat est assis sur le tapis.\" -> METEOR: 0.0000\n"
     ]
    }
   ],
   "source": [
    "# 4.3 METEOR (Metric for Evaluation of Translation with Explicit ORdering)\n",
    "# Considers synonyms and stemming. Output is 0-1.\n",
    "# NLTK's single_meteor_score expects untokenized strings.\n",
    "print(\"\\n--- METEOR Scores ---\")\n",
    "for target in target_sentences:\n",
    "    # METEOR is sensitive to tokenization; NLTK's implementation handles it.\n",
    "    # It's generally better for English.\n",
    "    try:\n",
    "        # For non-English or very different sentences, METEOR might be low or error if word alignment fails.\n",
    "        meteor_val = single_meteor_score(source_sentence, target)\n",
    "    except Exception as e: # Can sometimes have issues with very dissimilar/non-alpha sentences\n",
    "        meteor_val = 0.0\n",
    "        print(f\"Note: METEOR encountered an issue with '{target}', score set to 0. Error: {e}\")\n",
    "    print(f\"Target: \\\"{target}\\\" -> METEOR: {meteor_val:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1d20dc88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- chrF Scores ---\n",
      "Target: \"A feline was resting on the rug.\" -> chrF: 18.01\n",
      "Target: \"The cat was on the mat.\" -> chrF: 64.69\n",
      "Target: \"There is a cat on the mat.\" -> chrF: 64.21\n",
      "Target: \"The dog chased the ball.\" -> chrF: 16.73\n",
      "Target: \"Weather is pleasant today.\" -> chrF: 17.94\n",
      "Target: \"Le chat est assis sur le tapis.\" -> chrF: 12.12\n"
     ]
    }
   ],
   "source": [
    "# 4.4 chrF (character n-gram F-score)\n",
    "# Good for morphological variations and less sensitive to tokenization issues. Output is 0-100 (higher is better).\n",
    "print(\"\\n--- chrF Scores ---\")\n",
    "for target in target_sentences:\n",
    "    # sacrebleu.sentence_chrf expects untokenized strings.\n",
    "    # It takes the candidate first, then a list of references.\n",
    "    chrf_score = sacrebleu.sentence_chrf(target, [source_sentence]).score\n",
    "    print(f\"Target: \\\"{target}\\\" -> chrF: {chrf_score:.2f}\") # Score is typically 0-100"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8b122e4",
   "metadata": {},
   "source": [
    "# Embedding-based Similarity Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a3b0aafa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Sentence-BERT Similarity ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
      "WARNING:huggingface_hub.file_download:Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target: \"A feline was resting on the rug.\" -> SBERT Cosine Sim: 0.5631\n",
      "Target: \"The cat was on the mat.\" -> SBERT Cosine Sim: 0.9177\n",
      "Target: \"There is a cat on the mat.\" -> SBERT Cosine Sim: 0.9126\n",
      "Target: \"The dog chased the ball.\" -> SBERT Cosine Sim: 0.1213\n",
      "Target: \"Weather is pleasant today.\" -> SBERT Cosine Sim: -0.0260\n",
      "Target: \"Le chat est assis sur le tapis.\" -> SBERT Cosine Sim: 0.0319\n"
     ]
    }
   ],
   "source": [
    "# 5.1 Sentence-BERT (Sentence Transformers)\n",
    "# Calculates cosine similarity between sentence embeddings. Output is -1 to 1 (higher is better).\n",
    "print(\"\\n--- Sentence-BERT Similarity ---\")\n",
    "# You can choose different models from huggingface.co/models?library=sentence-transformers\n",
    "# 'all-MiniLM-L6-v2' is fast and good. 'all-mpnet-base-v2' is more robust.\n",
    "sbert_model_name = 'all-MiniLM-L6-v2'\n",
    "try:\n",
    "    sbert_model = SentenceTransformer(sbert_model_name)\n",
    "    source_embedding = sbert_model.encode(source_sentence, convert_to_tensor=True)\n",
    "    for target in target_sentences:\n",
    "        target_embedding = sbert_model.encode(target, convert_to_tensor=True)\n",
    "        cosine_similarity = util.pytorch_cos_sim(source_embedding, target_embedding).item()\n",
    "        print(f\"Target: \\\"{target}\\\" -> SBERT Cosine Sim: {cosine_similarity:.4f}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error loading SentenceTransformer model {sbert_model_name}: {e}\")\n",
    "    print(\"Skipping Sentence-BERT.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22d5b0c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- BERTScore F1 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
      "WARNING:huggingface_hub.file_download:Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target: \"A feline was resting on the rug.\" -> BERTScore F1: 0.9447\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target: \"The cat was on the mat.\" -> BERTScore F1: 0.9728\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target: \"There is a cat on the mat.\" -> BERTScore F1: 0.9496\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target: \"The dog chased the ball.\" -> BERTScore F1: 0.9350\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target: \"Weather is pleasant today.\" -> BERTScore F1: 0.8659\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target: \"Le chat est assis sur le tapis.\" -> BERTScore F1: 0.8546\n"
     ]
    }
   ],
   "source": [
    "# 5.2 BERTScore\n",
    "# Computes similarity by matching token embeddings from BERT, weighted by IDF.\n",
    "# Returns Precision, Recall, and F1. We'll use F1. Output is 0-1 (higher is better).\n",
    "print(\"\\n--- BERTScore F1 ---\")\n",
    "# BERTScore can be slow for many pairs without a GPU.\n",
    "# It automatically uses a default BERT model (can be specified).\n",
    "# It expects lists of candidates and references.\n",
    "try:\n",
    "    # bert_score_calc returns (P, R, F1) tensors\n",
    "    # We calculate it one by one for clarity here, but batching is more efficient.\n",
    "    for target in target_sentences:\n",
    "        P, R, F1 = bert_score_calc([target], [source_sentence], lang=\"en\", verbose=False, idf=False) # Disable IDF for pure semantic similarity\n",
    "        # For more robust scores, across domains,  idf=True or idf_sents=[list of sentences for corpus stats]\n",
    "        print(f\"Target: \\\"{target}\\\" -> BERTScore F1: {F1.mean():.4f}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error calculating BERTScore: {e}\")\n",
    "    print(\"Skipping BERTScore. Ensure you have a compatible PyTorch version and transformers.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "960f2cf9",
   "metadata": {},
   "source": [
    "# LLM-based Similarity (Prompt Engineering)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a14ca02a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- LLM (FLAN-T5-small) Prompted Similarity ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
      "WARNING:huggingface_hub.file_download:Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
      "WARNING:huggingface_hub.file_download:Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded LLM: google/flan-t5-small\n",
      "Target: \"A feline was resting on the rug.\" -> LLM Similarity: 1.0\n",
      "Target: \"The cat was on the mat.\" -> LLM Similarity: 1.0\n",
      "Target: \"There is a cat on the mat.\" -> LLM Similarity: 0.0\n",
      "Target: \"The dog chased the ball.\" -> LLM Similarity: 1.0\n",
      "Target: \"Weather is pleasant today.\" -> LLM Similarity: 1.0\n",
      "Target: \"Le chat est assis sur le tapis.\" -> LLM Similarity: 1.0\n",
      "\n",
      "Note: LLM-based scores are highly dependent on the prompt and model capabilities.\n",
      "The parsing of the score is also heuristic and might need refinement.\n"
     ]
    }
   ],
   "source": [
    "# 6. LLM-based Similarity (FLAN-T5-small)\n",
    "print(\"\\n--- LLM (FLAN-T5-small) Prompted Similarity ---\")\n",
    "llm_model_name = \"google/flan-t5-small\" \n",
    "\n",
    "try:\n",
    "    tokenizer_llm = AutoTokenizer.from_pretrained(llm_model_name)\n",
    "    model_llm = AutoModelForSeq2SeqLM.from_pretrained(llm_model_name)\n",
    "\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model_llm.to(device)\n",
    "    print(f\"Loaded LLM: {llm_model_name}\")\n",
    "\n",
    "    def get_llm_similarity(sentence1, sentence2):\n",
    "        prompt = f\"\"\"\n",
    "        Sentence 1: \"{sentence1}\"\n",
    "        Sentence 2: \"{sentence2}\"\n",
    "        Question: How semantically similar are Sentence 1 and Sentence 2?\n",
    "        Provide a similarity score from 0.0 (not similar) to 1.0 (identical in meaning).\n",
    "        Answer (Score only, e.g., 0.75):\n",
    "        \"\"\"\n",
    "        inputs = tokenizer_llm(prompt, return_tensors=\"pt\", max_length=512, truncation=True)\n",
    "        inputs = inputs.to(device)\n",
    "\n",
    "        outputs = model_llm.generate(**inputs, max_new_tokens=10) # Generate a short response\n",
    "        response_text = tokenizer_llm.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "        # [!] Attempt to parse the score from the response\n",
    "        match = re.search(r\"(\\d\\.\\d+)\", response_text)\n",
    "        if match:\n",
    "            try:\n",
    "                return float(match.group(1))\n",
    "            except ValueError:\n",
    "                return f\"Could not parse float from: {response_text}\"\n",
    "        else:\n",
    "            return f\"Could not find score in: {response_text}\"\n",
    "\n",
    "    for target in target_sentences:\n",
    "        llm_score = get_llm_similarity(source_sentence, target)\n",
    "        print(f\"Target: \\\"{target}\\\" -> LLM Similarity: {llm_score}\")\n",
    "    print(\"\\nNote: LLM-based scores are highly dependent on the prompt and model capabilities.\")\n",
    "    print(\"The parsing of the score is also heuristic and might need refinement.\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Error with LLM model {llm_model_name}: {e}\")\n",
    "    print(\"Skipping LLM-based similarity. Ensure transformers and a model are correctly set up.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f23c2ce",
   "metadata": {},
   "source": [
    "# Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fc47dc5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting result consolidation...\n",
      "\n",
      "Processing target 1/6: \"A feline was resting on the ru...\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing target 2/6: \"The cat was on the mat....\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing target 3/6: \"There is a cat on the mat....\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing target 4/6: \"The dog chased the ball....\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing target 5/6: \"Weather is pleasant today....\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing target 6/6: \"Le chat est assis sur le tapis...\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "--- Consolidated Results Table ---\n",
      "        Target Sentence           BLEU ROUGE-L   METEOR   chrF SBERT  BERTScore F1  LLM Sim (FLAN)\n",
      "A feline was resting on the rug. 0.070  0.308  Error/Low 18.01  0.563    0.945           1.0      \n",
      "         The cat was on the mat. 0.254  0.833  Error/Low 64.69  0.918    0.973           1.0      \n",
      "      There is a cat on the mat. 0.176  0.615  Error/Low 64.21  0.913    0.950           0.0      \n",
      "        The dog chased the ball. 0.052  0.364  Error/Low 16.73  0.121    0.935           1.0      \n",
      "      Weather is pleasant today. 0.000  0.000  Error/Low 17.94 -0.026    0.866           1.0      \n",
      " Le chat est assis sur le tapis. 0.000  0.000  Error/Low 12.12  0.032    0.855           1.0      \n",
      "\n",
      "Processing complete.\n"
     ]
    }
   ],
   "source": [
    "# 7. Consolidating Results (Fully Functional)\n",
    "\n",
    "import pandas as pd\n",
    "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
    "from nltk.translate.meteor_score import single_meteor_score\n",
    "from rouge_score import rouge_scorer\n",
    "import sacrebleu\n",
    "from sentence_transformers import util # Ensure util is imported if sbert_model is used\n",
    "# bert_score_calc should be imported from bert_score import score as bert_score_calc\n",
    "# Ensure get_llm_similarity is defined from the previous cell\n",
    "\n",
    "results_summary = []\n",
    "smoothie = SmoothingFunction().method1 # For BLEU\n",
    "\n",
    "# Define a rouge scorer instance once\n",
    "rouge_l_scorer = rouge_scorer.RougeScorer(['rougeL'], use_stemmer=True)\n",
    "\n",
    "print(\"Starting result consolidation...\\n\")\n",
    "\n",
    "for i, target in enumerate(target_sentences):\n",
    "    print(f\"Processing target {i+1}/{len(target_sentences)}: \\\"{target[:30]}...\\\"\")\n",
    "    current_scores = {\"Target Sentence\": target}\n",
    "    normalized_target_for_bleu = normalize_text(target).split()\n",
    "    normalized_source_for_bleu = normalize_text(source_sentence).split()\n",
    "\n",
    "    # BLEU\n",
    "    try:\n",
    "        bleu_val = sentence_bleu([normalized_source_for_bleu], normalized_target_for_bleu, smoothing_function=smoothie)\n",
    "        current_scores[\"BLEU\"] = f\"{bleu_val:.3f}\"\n",
    "    except Exception as e:\n",
    "        current_scores[\"BLEU\"] = f\"Error: {e}\"\n",
    "\n",
    "    # ROUGE-L\n",
    "    try:\n",
    "        rouge_scores = rouge_l_scorer.score(source_sentence, target)\n",
    "        current_scores[\"ROUGE-L\"] = f\"{rouge_scores['rougeL'].fmeasure:.3f}\"\n",
    "    except Exception as e:\n",
    "        current_scores[\"ROUGE-L\"] = f\"Error: {e}\"\n",
    "\n",
    "    # METEOR\n",
    "    try:\n",
    "        meteor_val = single_meteor_score(source_sentence, target)\n",
    "        current_scores[\"METEOR\"] = f\"{meteor_val:.3f}\"\n",
    "    except Exception as e: # Handles cases like ZeroDivisionError for very dissimilar sentences or non-alpha\n",
    "        current_scores[\"METEOR\"] = \"Error/Low\" # Or f\"{e}\" for specific error\n",
    "\n",
    "    # chrF\n",
    "    try:\n",
    "        chrf_val = sacrebleu.sentence_chrf(target, [source_sentence]).score\n",
    "        current_scores[\"chrF\"] = f\"{chrf_val:.2f}\" # chrF score is 0-100\n",
    "    except Exception as e:\n",
    "        current_scores[\"chrF\"] = f\"Error: {e}\"\n",
    "\n",
    "    # Sentence-BERT\n",
    "    try:\n",
    "        # Check if sbert_model was loaded successfully in the previous cell\n",
    "        if 'sbert_model' in globals() and sbert_model is not None:\n",
    "            source_emb = sbert_model.encode(source_sentence, convert_to_tensor=True)\n",
    "            target_emb = sbert_model.encode(target, convert_to_tensor=True)\n",
    "            sbert_sim = util.pytorch_cos_sim(source_emb, target_emb).item()\n",
    "            current_scores[\"SBERT\"] = f\"{sbert_sim:.3f}\"\n",
    "        else:\n",
    "            current_scores[\"SBERT\"] = \"Skipped (Model not loaded)\"\n",
    "    except Exception as e:\n",
    "        current_scores[\"SBERT\"] = f\"Error: {e}\"\n",
    "\n",
    "    # BERTScore F1\n",
    "    try:\n",
    "        # bert_score_calc should be available if imports were successful\n",
    "        P, R, F1 = bert_score_calc([target], [source_sentence], lang=\"en\", verbose=False, idf=False)\n",
    "        current_scores[\"BERTScore F1\"] = f\"{F1.mean():.3f}\"\n",
    "    except NameError:\n",
    "        current_scores[\"BERTScore F1\"] = \"Skipped (bert_score_calc not found)\"\n",
    "    except Exception as e:\n",
    "        current_scores[\"BERTScore F1\"] = f\"Error: {e}\"\n",
    "\n",
    "\n",
    "    # LLM Similarity (FLAN-T5)\n",
    "    try:\n",
    "        # Check if LLM models and function were loaded/defined\n",
    "        if ('model_llm' in globals() and model_llm is not None and\n",
    "            'tokenizer_llm' in globals() and tokenizer_llm is not None and\n",
    "            'get_llm_similarity' in globals()):\n",
    "            llm_sim = get_llm_similarity(source_sentence, target)\n",
    "            current_scores[\"LLM Sim (FLAN)\"] = llm_sim # llm_sim might already be a string (score or error message)\n",
    "        else:\n",
    "            current_scores[\"LLM Sim (FLAN)\"] = \"Skipped (Model/Func not loaded)\"\n",
    "    except Exception as e:\n",
    "        current_scores[\"LLM Sim (FLAN)\"] = f\"Error: {e}\"\n",
    "\n",
    "    results_summary.append(current_scores)\n",
    "\n",
    "df_results = pd.DataFrame(results_summary)\n",
    "\n",
    "# Set pandas display options for neat printing\n",
    "pd.set_option('display.max_rows', None)\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.width', 1000) # Adjust width as needed for your display\n",
    "pd.set_option('display.colheader_justify', 'center')\n",
    "pd.set_option('display.precision', 3) # For float formatting if not already string\n",
    "\n",
    "print(\"\\n\\n--- Consolidated Results Table ---\")\n",
    "print(df_results.to_string(index=False))\n",
    "\n",
    "print(\"\\nProcessing complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "746ea231",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
